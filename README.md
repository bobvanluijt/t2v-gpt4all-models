# gpt4all inference (for Weaviate)

This is the the inference container which is used by the Weaviate
`text2vec-gpt4all` module. You can download it directly from Dockerhub
using one of the pre-built images or built your own (as outlined below).

It is built in a way to support basic CPU model inference from your disk.

This makes this an easy way to deploy your Weaviate-optimized CPU
NLP inference model to production using Docker or Kubernetes.

## Documentation

Documentation for this module can be found [here](https://weaviate.io/developers/weaviate/current/retriever-vectorizer-modules/text2vec-gpt4all.html).
